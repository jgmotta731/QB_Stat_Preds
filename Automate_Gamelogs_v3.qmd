---
title: "Automate Gamelogs"
author: "Jack Motta"
format: 
  html:
    embed-resources: true
    number-sections: true
    toc: true
editor: visual
---

```{r, warning=FALSE, message=FALSE}
#| message: false
#| warning: false
library(tidyverse)
library(plotly)
library(splines)
library(nflverse)
library(gsisdecoder)
library(rsample)
library(nflplotR)
library(NFLSimulatoR)
library(arrow)
library(caret)
library(png)
library(glmnet)
library(tidymodels)
library(finetune)
library(doParallel)
library(tictoc)
library(bslib)
library(reactable)
library(reactablefmtr)
library(knitr)
library(rvest)
library(purrr)
library(kableExtra)
library(lubridate)
library(robotstxt)
library(janitor)
library(polite)
library(nflreadr)
library(future)
library(future.callr)
library(mice)
library(VIM)
library(naniar)
library(tidymodels)
library(stacks)
library(httr)
library(furrr)
library(car)
library(corrplot)
library(zoo)
library(forecast)
library(factoextra)
library(janitor)
library(tidytext)
library(foreach)
library(vip)
library(rstanarm)
library(rpart)
library(brms)
library(poissonreg)
library(nnet)
library(doFuture)
library(progressr)
library(doRNG)
library(randomForest)
library(gt)
```

# Data Acquisition

## QB Stats

```{r, warning=FALSE, message=FALSE}
tic()
####################################### Offense Stats ##################################
# Load weekly offensive stats (and filter for only QBs)
offense <- load_player_stats(2017:most_recent_season(T), "offense") %>%
  filter(position == "QB") %>%
  select(-player_name, -position, -position_group) %>%
  rename(pass_attempts = attempts,
         rush_attempts = carries) %>%
  mutate(across(player_display_name, clean_player_names),
         across(c(recent_team, opponent_team), clean_team_abbrs)) %>%
  select(-starts_with("rec"))

###################################### Advanced Stats ##################################

##### Advanced Passing #####
adv_pass <- load_pfr_advstats(2018:most_recent_season(T), stat_type = "pass", 
                              summary_level = "week") %>%
  select(where(~ !all(is.na(.)))) %>%
  rename(player_display_name = pfr_player_name,
         recent_team = team,
         opponent_team = opponent,
         season_type = game_type) %>%
  mutate(
    across(c(player_display_name), clean_player_names),
    across(c(recent_team, opponent_team), clean_team_abbrs),
    season_type = case_when(
      season_type %in% c("CON", "DIV", "SB", "WC") ~ "POST",  # Replace playoff types with "POST"
      TRUE ~ season_type
      )
    )

##### Advanced Rushing #####
adv_rush <- load_pfr_advstats(2018:most_recent_season(T), stat_type = "rush", 
                              summary_level = "week") %>%
  select(where(~ !all(is.na(.)))) %>%
  rename(player_display_name = pfr_player_name,
         recent_team = team,
         opponent_team = opponent,
         season_type = game_type) %>%
  mutate(
    across(c(player_display_name), clean_player_names),
    across(c(recent_team, opponent_team), clean_team_abbrs),
    season_type = case_when(
      season_type %in% c("CON", "DIV", "SB", "WC") ~ "POST", # Make playoff games POST
      TRUE ~ season_type
      )
    )

##### Join Adv Pass and Adv Rush Stats #####
adv_qb_stats <- adv_pass %>%
  left_join(adv_rush, 
            by = c("player_display_name", "season", "game_id", "pfr_game_id", "week", 
                   "season_type", "recent_team", "opponent_team", "pfr_player_id")) %>%
  rename(rush_attempts = carries) %>%
  select(-pfr_game_id, -pfr_player_id)

#################################### Play-By-Play Data #################################
pbp_cores <- most_recent_season(roster = FALSE)-2017
cl <- makeCluster(pbp_cores)     # Set cores to number of seasons    
registerDoParallel(cl)           # Register the parallel backend
pbp_refresh <- load_pbp(2017:most_recent_season(roster = FALSE))
stopCluster(cl)                  # Stop the cluster
registerDoSEQ()                  # Reset to sequential processing
write_parquet(pbp_refresh, "pbp_refresh.parquet") # Efficient to read in
pbp <- read_parquet("pbp_refresh.parquet")

# Create game_info Contextual Data at the Game Level
game_info <- pbp %>%
  group_by(game_id, season, week) %>%
  summarize(
    # Use distinct game-level attributes
    home_team = first(home_team),
    away_team = first(away_team),
    season_type = first(season_type),
    stadium = first(stadium),
    roof = first(roof),
    surface = first(surface),
    game_date = first(game_date),
    home_coach = first(home_coach),
    away_coach = first(away_coach),
  ) %>%
  ungroup()

##################################### Birth Dates ######################################
birth_dates <- load_rosters(2017:most_recent_season(T)) %>%
  rename(player_display_name = full_name,
         recent_team = team) %>%
  filter(position == "QB") %>%
  mutate(across(player_display_name, clean_player_names),
         across(recent_team, clean_team_abbrs)) %>%
  select(player_display_name, birth_date, season, years_exp, entry_year, status)

###################################### ESPN QBR ########################################
espn_qbr <- load_espn_qbr(2017:most_recent_season(T), summary_type = "week") %>%
  filter(season >= 2017) %>%
  rename(player_display_name = name_display,
         week = game_week,
         epa_penalty = penalty,
         epa_sack = sack,
         epa_pass = pass,
         epa_run = run,
         opponent_team = opp_abb) %>%
  mutate(across(player_display_name, clean_player_names),
         across(opponent_team, clean_team_abbrs),
           # Update season_type column
         season_type = case_when(
           season_type == "Regular" ~ "REG",
           season_type == "Playoffs" ~ "POST",
           TRUE ~ season_type), # Keep other values unchanged
         # Update week column based on conditions
         week = case_when(
           # For seasons 2021 and greater
           season >= 2021 & week_text == "Wild Card" ~ 19,
           season >= 2021 & week_text == "Divisional Round" ~ 20,
           season >= 2021 & week_text == "Conference Championship" ~ 21,
           season >= 2021 & week_text == "Super Bowl" ~ 22,
           # For seasons less than 2021
           season < 2021 & week_text == "Wild Card" ~ 18,
           season < 2021 & week_text == "Divisional Round" ~ 19,
           season < 2021 & week_text == "Conference Championship" ~ 20,
           season < 2021 & week_text == "Super Bowl" ~ 21,
           # Keep week unchanged for other cases
           TRUE ~ week
           )) %>%
  select(season, week, qbr_total, epa_total, epa_pass, epa_run, epa_sack, epa_penalty, qbr_raw, player_display_name, qb_plays, pts_added, opponent_team)

espn_qbr_2 <- read.csv("qbr_missing_cleaned.csv") %>%
  filter(season >= 2017) %>%
  rename(player_display_name = name_display,
         recent_team = team_abb,
         epa_penalty = penalty,
         epa_sack = exp_sack,
         epa_pass = pass,
         epa_run = run,
         opponent_team = opp_abb) %>%
  select(season, week, qbr_total, epa_total, epa_pass, epa_run, epa_sack, epa_penalty, qbr_raw, player_display_name, qb_plays, pts_added, opponent_team) %>%
  mutate(across(player_display_name, clean_player_names),
         across(opponent_team, clean_team_abbrs))

espn_qbr_joined <- espn_qbr %>%
  full_join(espn_qbr_2, by = c("player_display_name", "season", "week", "qbr_total", "epa_pass", "epa_run", "epa_sack", "epa_penalty", "qbr_raw", "qb_plays", "pts_added", "epa_total", "opponent_team"))

################################# NextGen Passing ######################################
nextgen_pass <- load_nextgen_stats(2017:most_recent_season(T), stat_type = "passing") %>%
  filter(player_position == "QB") %>%
  filter(week != 0) %>%
  select(-player_last_name, -player_first_name, -player_short_name, -player_position) %>%
  rename(recent_team = team_abbr,
         pass_attempts = attempts,
         passing_yards = pass_yards,
         passing_tds = pass_touchdowns) %>%
  mutate(
    across(player_display_name, clean_player_names),
    recent_team = case_when(
    season == 2021 & player_display_name == "Kirk Cousins"    ~ "MIN",
    season == 2021 & player_display_name == "Aaron Rodgers"   ~ "GB",
    season == 2021 & player_display_name == "Justin Herbert"  ~ "LAC",
    season == 2021 & player_display_name == "Kyler Murray"    ~ "ARI",
    season == 2021 & player_display_name == "Russell Wilson"  ~ "SEA",
    season == 2021 & player_display_name == "Patrick Mahomes" ~ "KC",
    season == 2021 & player_display_name == "Lamar Jackson"   ~ "BAL",
    season == 2021 & player_display_name == "Tom Brady"       ~ "TB",
    TRUE ~ recent_team  # Retain original value if not in the list or not 2021
    ),
    across(recent_team, clean_team_abbrs))


################################# NextGen Rushing ######################################
nextgen_rush <- load_nextgen_stats(2017:most_recent_season(T), stat_type = "rushing") %>%
  filter(player_position == "QB") %>%
  filter(week != 0) %>%
  select(-player_last_name, -player_first_name, -player_short_name, -player_jersey_number, -player_position) %>%
  rename(recent_team = team_abbr,
         rushing_yards = rush_yards,
         rushing_tds = rush_touchdowns) %>%
  mutate(
    across(player_display_name, clean_player_names),
    across(recent_team, clean_team_abbrs))

##################################### Joining NextGen ##################################
nextgen_stats <- nextgen_pass %>%
  left_join(nextgen_rush, by = c("season", "season_type", "week", "player_display_name", 
                                 "recent_team", "player_gsis_id"))

#################################### Load Active Rosters ###############################
active_qbs <- load_rosters_weekly(2017:most_recent_season(T)) %>%  # Explicitly specify 2017:2024
  rename(recent_team = team,
         player_display_name = full_name,
         season_type = game_type) %>%
  filter(position == "QB") %>%  # Filter for quarterbacks
  filter(season == most_recent_season(T), status == "ACT") %>% # Find QBs active in 2024
  select(season, recent_team, status, player_display_name, week, season_type) %>%
  mutate(across(player_display_name, clean_player_names),
         across(recent_team, clean_team_abbrs)) %>%
  pull(player_display_name)

# Filtering to only include most recent active QBs
weekly_rosters <- load_rosters_weekly(2017:most_recent_season(T)) %>%
  rename(recent_team = team,
         player_display_name = full_name) %>%
  mutate(across(player_display_name, clean_player_names),
         across(recent_team, clean_team_abbrs)) %>%
  filter(position == "QB") %>%  # Filter for quarterbacks
  filter(status == "ACT") %>%  # Filter for active players
  filter(player_display_name %in% active_qbs) %>%  # Filter for QBs active in most recent season
  select(season, week, player_display_name, recent_team) %>%  # Select relevant columns
  arrange(season, week, player_display_name)
```

### Joining

```{r, warning=FALSE, message=FALSE}
qb_stats <- offense %>%
  left_join(espn_qbr_joined, by = c("player_display_name", "season", "week", "opponent_team")) %>%
  filter(player_display_name %in% active_qbs) %>%
  left_join(weekly_rosters, by = c("player_display_name", "season", "week")) %>%
  left_join(birth_dates, by = c("player_display_name", "season")) %>%
  left_join(nextgen_stats, by = c("player_display_name", "season", "week", "season_type", "recent_team")) %>%
  left_join(adv_qb_stats, by = c("player_display_name", "season", "week", "season_type", "opponent_team", "recent_team")) %>%
  mutate(rush_attempts = coalesce(rush_attempts.x, rush_attempts.y)) %>% # Fill `rush_attempts` NA values
  select(-rush_attempts.x, -rush_attempts.y) %>%  # Remove duplicate columns
  select(-player_jersey_number, -player_gsis_id, -targets, -target_share, -racr, -wopr,
         -air_yards_share, -special_teams_tds, -times_sacked) %>%
  rename(times_sacked = sacks) %>%
  left_join(game_info, by = c("game_id", "season", "week", "season_type")) %>%
  left_join(game_info, by = c("season", "week", "recent_team" = "home_team"), 
            suffix = c("", ".info")) %>%
  mutate(
    # Fill missing values from game_info where recent_team is home_team
    game_id = coalesce(game_id, game_id.info),
    away_team = coalesce(away_team, away_team.info),
    home_coach = coalesce(home_coach, home_coach.info),
    away_coach = coalesce(away_coach, away_coach.info),
    roof = coalesce(roof, roof.info),
    stadium = coalesce(stadium, stadium.info),
    surface = coalesce(surface, surface.info),
    game_date = coalesce(game_date, game_date.info)
  ) %>%
  # Drop the .info columns after filling in values
  select(-ends_with(".info"))
qb_stats <- qb_stats %>%
  left_join(game_info, by = c("season", "week", "recent_team" = "away_team"), 
            suffix = c("", ".info")) %>%
  mutate(
    # Fill remaining missing values from game_info where recent_team is away_team
    game_id = coalesce(game_id, game_id.info),
    home_team = coalesce(home_team, home_team.info),
    home_coach = coalesce(home_coach, home_coach.info),
    away_coach = coalesce(away_coach, away_coach.info),
    roof = coalesce(roof, roof.info),
    stadium = coalesce(stadium, stadium.info),
    surface = coalesce(surface, surface.info),
    game_date = coalesce(game_date, game_date.info),
  ) %>%
  # Remove extra columns with .info suffix
  select(-ends_with(".info")) %>%
  # Step 2: Coalesce all matching .x and .y columns
  mutate(
    across(
    .cols = intersect(names(qb_stats), paste0(names(qb_stats), ".y")),
    ~ coalesce(.x, get(paste0(cur_column(), ".y")))
  )) %>%
  # Remove `.y` columns after `coalesce()`
  select(-ends_with(".y")) %>%
  # Rename columns to remove `.x` suffix
  rename_with(~ sub("\\.x$", "", .), ends_with(".x")) %>%
  select(-ends_with(".x"), -ends_with(".y")) %>%
  arrange(season, week, player_display_name) %>%
  relocate(player_id, player_display_name, recent_team, season, week, game_id, opponent_team, game_date) %>%
  # Calculate passer rating for each player in the dataset
  mutate(
    total_fumbles = coalesce(sack_fumbles, 0) + coalesce(rushing_fumbles, 0),
    completion_percentage = completions / pass_attempts,  # Completion Percentage
    yards_per_attempt = passing_yards / pass_attempts,  # Yards per Attempt
    touchdown_pct = passing_tds / pass_attempts,  # Touchdown Percentage
    interception_pct = interceptions / pass_attempts,  # Interception Percentage

    # Passer rating components
    C = pmax(0, pmin((completion_percentage - 0.3) * 5, 2.375)),
    Y = pmax(0, pmin((yards_per_attempt - 3) * 0.25, 2.375)),
    t = pmax(0, pmin(touchdown_pct * 20, 2.375)),
    I = pmax(0, pmin(2.375 - (interception_pct * 25), 2.375)),

    # Update passer_rating only where it is NA
    passer_rating = coalesce(
      passer_rating,  # Keep existing values if not NA
      ((C + Y + t + I) / 6) * 100  # Fill NA with calculated value
    )
  ) %>%
  select(-C, -Y, -t, -I) %>% # Remove intermediate columns
  # Step 3
  select(where(~ !all(is.na(.)))) %>%
  mutate(
    # Impute completion_percentage using completions and pass_attempts
    completion_percentage = ifelse(
      is.na(completion_percentage) & pass_attempts > 0,
      completions / pass_attempts,
      completion_percentage
    ),
    
    # Impute yards_per_attempt using passing_yards and pass_attempts
    yards_per_attempt = ifelse(
      is.na(yards_per_attempt) & pass_attempts > 0,
      passing_yards / pass_attempts,
      yards_per_attempt
    ),
    
    # Impute touchdown_pct using passing_tds and pass_attempts
    touchdown_pct = ifelse(
      is.na(touchdown_pct) & pass_attempts > 0,
      passing_tds / pass_attempts,
      touchdown_pct
    ),
    
    # Impute interception_pct using interceptions and pass_attempts
    interception_pct = ifelse(
      is.na(interception_pct) & pass_attempts > 0,
      interceptions / pass_attempts,
      interception_pct
    ),
    # Impute passing_air_yards and passing_yards_after_catch
    passing_air_yards = ifelse(
      is.na(passing_air_yards) & !is.na(passing_yards_after_catch),
      passing_yards - passing_yards_after_catch,
      passing_air_yards
    ),
    passing_yards_after_catch = ifelse(
      is.na(passing_yards_after_catch) & !is.na(passing_air_yards),
      passing_yards - passing_air_yards,
      passing_yards_after_catch
    ),
    
    # Impute completion_percentage_above_expectation
    completion_percentage_above_expectation = ifelse(
      is.na(completion_percentage_above_expectation) & !is.na(completion_percentage) 
      & !is.na(expected_completion_percentage),
      completion_percentage - expected_completion_percentage,
      completion_percentage_above_expectation
    ),
    
    # Impute passing_2pt_conversions
    passing_2pt_conversions = ifelse(
      is.na(passing_2pt_conversions) & !is.na(passing_epa),
      ifelse(passing_epa > 0, 1, 0),  # Assume conversion if EPA > 0
      passing_2pt_conversions
    ),
    
    # Impute rushing_epa
    rushing_epa = ifelse(
      is.na(rushing_epa) & rushing_yards > 0,
      rushing_yards + 6 * rushing_tds - 3 * rushing_fumbles,  # Example weights
      rushing_epa
    ),
    
    # Impute qbr_total using qbr_raw
    qbr_total = ifelse(
      is.na(qbr_total) & !is.na(qbr_raw),
      qbr_raw * 1.1,  # Example scaling factor
      qbr_total
    ),
    passing_epa = ifelse(
      is.na(passing_epa), 
      passing_yards + 6 * passing_tds - 3 * interceptions, passing_epa),
      pacr = ifelse(is.na(pacr), 
      passing_yards / passing_air_yards, 
      pacr
      ),
    passing_drops = ifelse(
      is.na(passing_drops), 
      0, 
      passing_drops
      ),
    passing_drop_pct = ifelse(
      is.na(passing_drop_pct), 
      0, 
      passing_drop_pct
      ),
    recent_team = ifelse(
      is.na(
        recent_team), 
      ifelse(
        opponent_team == home_team, 
        away_team, 
        home_team),
      recent_team
      )
) %>%
  mutate(across(
    contains("rush"),
    ~ ifelse(rush_attempts == 0, 0, .),
    .names = "{.col}"
  )) %>%
  filter(pass_attempts + rush_attempts >= 20 | qb_plays >= 20)


# Dynamically identify percentage columns
percentage_columns <- colnames(qb_stats)[str_detect(colnames(qb_stats), regex("percent|pct|percentage", ignore_case = TRUE))]

# Define a function to standardize a column
standardize_percentage <- function(col) {
  if (any(abs(col) > 1, na.rm = TRUE)) {
    return(col / 100)  # Scale entire column if any value > 1
  }
  return(col)  # Leave column unchanged if already in decimal format
}

# Apply the function to percentage columns
qb_stats <- qb_stats %>%
  mutate(across(all_of(percentage_columns), standardize_percentage),
         passing_air_yards_per_attempt = passing_air_yards/pass_attempts)

# Check the result
#summary(qb_stats[percentage_columns])
```

## Data Cleaning

```{r}
# Specify imputation methods for the columns
mean_impute <- c(
  "avg_time_to_throw", "avg_completed_air_yards", "avg_intended_air_yards", 
  "avg_air_yards_differential", "avg_air_distance", "max_air_distance", 
  "completion_percentage_above_expectation",
  "passing_bad_throws", "times_blitzed", "times_hurried", "times_hit",
  "times_pressured", "rushing_yards_before_contact", "rushing_yards_after_contact", 
  "rushing_broken_tackles", "rushing_yards_before_contact_avg", "rushing_yards_after_contact_avg", "max_completed_air_distance", "avg_air_yards_to_sticks", "times_pressured_pct", "expected_completion_percentage", "aggressiveness", "passing_bad_throw_pct"
)

median_impute <- c(
  "qbr_total", "epa_total", "epa_pass", "epa_run", "epa_sack", "epa_penalty",
  "qbr_raw", "qb_plays", "pts_added", "yards_per_attempt", "touchdown_pct", "interception_pct"
)

qb_stats <- qb_stats %>%
  group_by(player_display_name) %>%
  mutate(across(all_of(mean_impute), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .))) %>%  # Mean Imputation
  mutate(across(all_of(median_impute), ~ ifelse(is.na(.), median(., na.rm = TRUE), .))) %>%  # Median Imputation
  ungroup()

# Verify that all NAs have been handled
colSums(is.na(qb_stats))

write_parquet(qb_stats, "qb_stats_cleaned.parquet")
```

```{r, warning=FALSE, message=FALSE}
# Set columns to do rolling avg on
rolling_avg_columns <- qb_stats %>%
  select(where(~ !all(is.na(.)))) %>%
  select(-where(is.character), -season, -week, -years_exp, -entry_year, -birth_date) %>%
  colnames()

# Columns to retain
retain_columns <- c(
  "player_id",
  "player_display_name",
  "age",
  "recent_team",
  "season",
  "season_type",
  "week",
  "game_id",
  "home_away",
  "opponent_team",
  "game_date",
  "completions",
  "pass_attempts",
  "passing_yards",
  "passing_tds",
  "interceptions",
  "rush_attempts",
  "rushing_yards",
  "rushing_tds",
  "stadium",
  "surface",
  "roof"
)

# Define a named vector of stadium-to-surface mapping
stadium_surface_mapping <- c(
  "Tottenham Hotspur Stadium" = "grass",
  "Levi'sÂ® Stadium" = "grass",
  "Wembley Stadium" = "grass",
  "Allianz Arena" = "grass",
  "GEHA Field at Arrowhead Stadium" = "grass",
  "Lucas Oil Stadium" = "fieldturf",
  "U.S. Bank Stadium" = "fieldturf",
  "Acrisure Stadium" = "grass",
  "Mercedes-Benz Stadium" = "fieldturf",
  "M&T Bank Stadium" = "grass",
  "MetLife Stadium" = "fieldturf",
  "Caesars Superdome" = "fieldturf",
  "Cleveland Browns Stadium" = "grass",
  "Lumen Field" = "fieldturf",
  "Gillette Stadium" = "fieldturf",
  "Soldier Field" = "grass",
  "SoFi Stadium" = "matrixturf",
  "Empower Field at Mile High" = "grass",
  "Lincoln Financial Field" = "grass",
  "Raymond James Stadium" = "grass",
  "NRG Stadium" = "matrixturf",
  "AT&T Stadium" = "matrixturf",
  "State Farm Stadium" = "grass",
  "Ford Field" = "fieldturf",
  "Paycor Stadium" = "fieldturf",
  "Highmark Stadium" = "a_turf",
  "Nissan Stadium" = "grass",
  "EverBank Stadium" = "grass",
  "Bank of America Stadium" = "fieldturf",
  "Arena Corinthians" = "grass"
)
#saveRDS(stadium_surface_mapping, "stadium_surface_mapping.rds")
```

### Final QB Stats

```{r, warning=FALSE, message=FALSE}
# Rookie baseline for when QBs have no prior data
rookie_baseline <- qb_stats %>%
  filter(years_exp == 0) %>%
  summarize(across(all_of(rolling_avg_columns), ~ mean(., na.rm = TRUE), .names = "rookie_baseline_{col}"))

# Consolidate preprocessing and calculations
# Add rolling averages, lags, and rookie handling
qb_stats_final <- qb_stats %>%
  # Remove columns where all values are NA
  select(where(~ !all(is.na(.)))) %>%
  # Apply rolling averages
  group_by(player_display_name) %>%
  arrange(season, week, game_date) %>%
  mutate(
    # Rolling averages
    across(
      all_of(rolling_avg_columns),
      ~ rollapply(.x, width = 3, FUN = mean, fill = NA, align = "right", partial = TRUE),
      .names = "rolling_{col}"
    ),
    # Lags for rolling averages
    across(rolling_avg_columns, ~ lag(.x, n = 1), .names = "lag_{.col}")
  ) %>%
  ungroup() %>%
  # Handle rookies with no prior data
  left_join(rookie_baseline, by = character()) %>%  # Join rookie baseline values
  mutate(
    across(
      starts_with("rolling_"),
      ~ ifelse(is.na(.) & years_exp == 0, get(paste0("rookie_baseline_", cur_column())), .)),
    across(rolling_avg_columns, ~ lag(.x, n = 1), .names = "lag_{.col}")) %>%
  ungroup() %>%
  # Fill NA values for team columns
  mutate(
    away_team = if_else(is.na(away_team) & !is.na(game_id), 
                        str_extract(game_id, "(?<=_)[A-Z]{2,3}(?=_[A-Z]{2,3}$)"), 
                        away_team),
    home_team = if_else(is.na(home_team) & !is.na(game_id), 
                        str_extract(game_id, "(?<=_)[A-Z]{2,3}$"), 
                        home_team),
    opponent_team = if_else(is.na(opponent_team), "MIA", opponent_team),
  ) %>%
  
  # Create home/away indicator
  mutate(
    home_away = case_when(
      recent_team == away_team ~ "Away",
      recent_team == home_team ~ "Home",
      TRUE ~ NA_character_
    ),
    # Calculate age as of the game date
    age = as.integer(floor(interval(birth_date, game_date) / years(1))),
    coach = case_when(
      recent_team == home_team ~ home_coach,
      recent_team == away_team ~ away_coach
    ),
    opponent_coach = case_when(
      opponent_team == home_team ~ home_coach,
      opponent_team == away_team ~ away_coach
    )
  ) %>%
  select(-home_coach, -away_coach, -home_team, -away_team, -entry_year, -years_exp) %>% # Drop original coach/team columns
  
  # Clean and standardize the weather and surface
  mutate(
    surface = case_when(
      game_id == "2020_17_BAL_CIN" ~ "fieldturf",
      surface == "" ~ NA_character_,
      surface == "grass " ~ "grass",
      TRUE ~ surface
    ),
    surface = ifelse(is.na(surface), stadium_surface_mapping[stadium], surface)) %>%
  # Round all numeric columns
  mutate(across(where(is.numeric), ~ round(.x, 2))) %>%
  select(all_of(retain_columns), starts_with("lag_"), starts_with("rolling_"))

qb_stats_final <- qb_stats_final %>%
  filter(season > 2017)

rows_with_nas <- qb_stats_final %>%
  filter(if_any(everything(), is.na))

qb_stats_final <- qb_stats_final %>%
  mutate(surface = ifelse(is.na(surface) & stadium %in% c("Commanders Field", 
                                                          "Estadio Azteca (Mexico City)"
                                                          ), "grass", surface))

write_parquet(qb_stats_final, "qb_stats.parquet")
```

## Team Defense Stats

```{r, warning=FALSE, message=FALSE}
# Load weekly defensive player stats
defense <- load_player_stats(2017:most_recent_season(T), "defense") %>%
  select(-season_type, -player_id, -position_group, -position, -headshot_url, -player_name) %>%
  rename(recent_team = team) %>%
  mutate(
    across(player_display_name, clean_player_names),
    across(recent_team, clean_team_abbrs)
  )

# Load advanced defensive player stats
adv_def <- load_pfr_advstats(2018:most_recent_season(T), stat_type = "def", summary_level = "week") %>%
  select(where(~ !all(is.na(.))), -game_type, -pfr_player_id, -pfr_game_id, -game_id, -opponent) %>%
  rename(player_display_name = pfr_player_name,
           recent_team = team) %>%
  mutate(
      across(c(player_display_name), clean_player_names),
      across(recent_team, clean_team_abbrs))

# Defense
defense_stats <- defense %>%
  full_join(adv_def, by = c("season", "week", "player_display_name", "recent_team"))

defense_stats <- defense_stats %>%
  mutate(
    across(
    .cols = intersect(names(defense_stats), paste0(names(defense_stats), ".y")),
    ~ coalesce(.x, get(paste0(cur_column(), ".y"))))) %>%
    # Remove `.y` columns after `coalesce()`
  select(-ends_with(".y")) %>%
  # Rename columns to remove `.x` suffix
  rename_with(~ sub("\\.x$", "", .), ends_with(".x")) %>%
  select(-ends_with(".x"), -ends_with(".y")) %>%
  arrange(season, week, player_display_name) %>%
  relocate(player_display_name, recent_team, season, week)

zero_impute_def <- c(
  "def_tackles", "def_tackles_solo", "def_tackles_with_assist", "def_tackle_assists",
  "def_tackles_for_loss", "def_tackles_for_loss_yards", "def_fumbles_forced", 
  "def_sacks", "def_sack_yards", "def_qb_hits", "def_interceptions", 
  "def_interception_yards", "def_pass_defended", "def_tds", "def_fumbles", 
  "def_fumble_recovery_own", "def_fumble_recovery_yards_own", 
  "def_fumble_recovery_opp", "def_fumble_recovery_yards_opp", 
  "def_safety", "def_penalty", "def_penalty_yards", "def_ints"
)

mean_impute_def <- c(
  "def_completion_pct", "def_yards_allowed_per_cmp", 
  "def_yards_allowed_per_tgt", "def_passer_rating_allowed", 
  "def_adot", "def_air_yards_completed", "def_yards_after_catch", 
  "def_missed_tackle_pct"
)

median_impute_def <- c(
  "def_targets", "def_completions_allowed", "def_yards_allowed", 
  "def_receiving_td_allowed", "def_times_blitzed", "def_times_hurried", 
  "def_times_hitqb", "def_pressures", "def_tackles_combined", 
  "def_missed_tackles"
)


defense_stats <- defense_stats %>%
  group_by(player_display_name) %>%
  mutate(
    # Zero Imputation
    across(
      all_of(zero_impute_def),
      ~ ifelse(is.na(.), 0, .)
    ),
    
    # Mean Imputation (fallback to 0 if mean cannot be computed)
    across(
      all_of(mean_impute_def),
      ~ ifelse(is.na(.), 
               ifelse(is.nan(mean(., na.rm = TRUE)) | is.infinite(mean(., na.rm = TRUE)), 0, mean(., na.rm = TRUE)), 
               .)
    ),
    
    # Median Imputation (fallback to 0 if median cannot be computed)
    across(
      all_of(median_impute_def),
      ~ ifelse(is.na(.), 
               ifelse(is.nan(median(., na.rm = TRUE)) | is.infinite(median(., na.rm = TRUE)), 0, median(., na.rm = TRUE)), 
               .)
    )
  ) %>%
  ungroup() %>%
  # Final fallback for any remaining NA values
  mutate(across(where(is.numeric), ~ ifelse(is.na(.), 0, .)))

colSums(is.na(defense_stats))
```

### Aggregating

```{r, warning=FALSE, message=FALSE}
# Calculate rolling averages and drop original columns
def_stats.opp <- defense_stats %>%
  # Exclude specific columns from summation
  select(-player_display_name) %>%
  group_by(recent_team, season, week) %>%
  summarise(
    across( where(is.numeric), sum, .names = "{col}", na.rm = TRUE), 
    .groups = "drop") %>%
  arrange(recent_team, season, week) %>%
  ungroup() %>%
  group_by(recent_team) %>%
  # Apply rolling means to summed columns (excluding specific ones)
  mutate(
    across(
      where(is.numeric),
      ~ rollapply(.x, width = 3, FUN = mean, fill = NA, align = "right", partial = TRUE),
      .names = "rolling_{col}"
    )
  ) %>%
  # Apply lagging to summed columns (excluding specific ones)
  mutate(
    across(
      where(is.numeric) & !starts_with("rolling_"),
      ~ lag(.x, n = 1),
      .names = "lag_{col}"
    )
  ) %>%
  ungroup() %>%
  # Keep only lagged and rolling columns
  select(season, week, recent_team, starts_with("lag_"), starts_with("rolling_"), -lag_week, -rolling_week, -lag_season, -rolling_season) %>%
  filter(season >= 2018) %>%
  rename(opponent_team = recent_team)

write_parquet(def_stats.opp, "opp_def_stats.parquet")
```

## Joining QBs and Opponent Defense Stats

```{r, warning=FALSE, message=FALSE}
def_stats.opp <- read_parquet("opp_def_stats.parquet")
qb_stats_final <- read_parquet("qb_stats.parquet")

qb_logs_new <- qb_stats_final %>%
  left_join(def_stats.opp, by = c("season", "week", "opponent_team")) %>%
  rename(team = recent_team,
         opponent = opponent_team,
         player_name = player_display_name) %>%
  select(-rolling_fantasy_points, -rolling_fantasy_points_ppr)

write_parquet(qb_logs_new, "qb_gamelogs_new.parquet")
```

## Filtering for Temporal Split

```{r, warning=FALSE, message=FALSE}
qb_logs_new <- read_parquet("qb_gamelogs_new.parquet")

# Filter out 2024 season
valid_qbs <- qb_logs_new %>%
  filter(season < most_recent_season(T)) %>% 
  group_by(player_name) %>%
  # Count the number of games played by each player
  mutate(games_played = n()) %>%
  ungroup() %>%
  # Retain only players with at least 8 games played
  filter(games_played >= 8)

# Filter to only include these players
qb_logs_valid <- qb_logs_new %>%
  filter(player_name %in% valid_qbs$player_name)

# Create list of starting QBs for upcoming games
starting_qbs <- load_schedules(most_recent_season(T)) %>%
  select(game_id, season, week, away_team, away_qb_name, home_team, home_qb_name) %>%
  mutate(across(away_qb_name, clean_player_names),
         across(home_qb_name, clean_player_names))

qb_logs_valid <- qb_logs_valid %>%
  mutate(
    is_valid = case_when(
      home_away == "Away" & player_name %in% starting_qbs$away_qb_name ~ TRUE,
      home_away == "Home" & player_name %in% starting_qbs$home_qb_name ~ TRUE,
      TRUE ~ FALSE
    )
  )

dim(qb_logs_valid)

write_parquet(qb_logs_valid, "qb_logs_valid.parquet")
toc()
rm(retain_columns, rolling_avg_columns, defense, def_stats.opp, defense_stats, offense, adv_def, adv_pass, adv_qb_stats, adv_rush, espn_qbr, starting_qbs, game_info, pbp, pbp_refresh, birth_dates, nextgen_pass, nextgen_rush, nextgen_stats, pbp_cores, stadium_surface_mapping, valid_qbs, qb_stats, qb_stats_final, qb_stats_final_filtered, cl)
```

# EDA

```{r, warning=FALSE, message=FALSE}
gamelogs <- read_parquet("qb_gamelogs_new.parquet") %>%
  arrange(game_date, game_id, season, week, player_name) %>%
  mutate(rushing_yards = ifelse(rushing_yards < 0, 0, rushing_yards)) %>%
  select(-rolling_def_ints)

# Select response variables
response_vars <- c("pass_attempts", "passing_yards", "passing_tds", 
                   "rush_attempts", "rushing_yards", "interceptions", 
                   "rushing_tds", "completions")

# Select numeric predictors and responses
numeric_data <- gamelogs %>%
  select(where(is.numeric))

# Compute correlation matrix
cor_matrix <- cor(numeric_data, use = "complete.obs")

# Reshape correlation matrix for easier filtering
cor_df <- as.data.frame(as.table(cor_matrix)) %>%
  rename(predictor = Var1, response = Var2, correlation = Freq)

# Filter for correlations involving response variables
filtered_cor_df <- cor_df %>%
  filter(response %in% response_vars & predictor != response) %>%
  arrange(response, desc(abs(correlation)), predictor) %>%  # Sort by absolute correlation strength
  #filter(abs(correlation) >= 0.5) %>% # Apply a threshold (e.g., abs(correlation) >= 0.4)
  filter(!(predictor %in% response_vars))

# Display top correlations
filtered_cor_df <- filtered_cor_df %>%
  arrange(response, desc(correlation), predictor)

# Line plots by season and week
gamelogs %>%
  group_by(season, week) %>%
  summarise(across(all_of(response_vars), mean, na.rm = TRUE)) %>%
  pivot_longer(cols = -c(season, week), names_to = "response", values_to = "mean_value") %>%
  ggplot(aes(x = week, y = mean_value, color = as.factor(season))) +
  geom_line() +
  facet_wrap(~response, scales = "free") +
  theme_minimal() +
  labs(title = "Weekly Trends of Response Variables", x = "Week", y = "Mean Value")
```

# Modeling

## Data Split

```{r}
gamelogs <- read_parquet("qb_gamelogs_new.parquet") %>%
  arrange(game_date, game_id, season, week, player_name) %>%
  mutate(rushing_yards = ifelse(rushing_yards < 0, 0, rushing_yards),
         rolling_rushing_yards = ifelse(rolling_rushing_yards < 0, 0, rolling_rushing_yards)) %>%
  select(-rolling_def_ints)


set.seed(6341)
# Create a single split for the dataset
player_splits <- make_splits(
  gamelogs %>% filter(season < most_recent_season(T)-1),
  gamelogs %>% filter(season >= most_recent_season(T)-1)
)

# Extract training and testing datasets
player_train <- training(player_splits)
player_test <- testing(player_splits)

# Calculate fold size
n <- nrow(player_train)
fold_size <- round(n / 10)

# Temporal Cross-Validation with 10 Folds
player_folds <- rolling_origin(
  data = player_train,
  initial = n - 9 * fold_size,  # Ensure enough data remains for 8 folds
  assess = fold_size,          # Each fold assesses on `fold_size`
  skip = fold_size,            # Skip `fold_size` between folds
  cumulative = TRUE            # Training set grows over time
)

# Put response variables in a vector
response_vars <- c("pass_attempts", "completions", "passing_yards", 
                   "rush_attempts", "rushing_yards", "passing_tds", 
                   "interceptions", "rushing_tds")
```

## Recipe

```{r, warning=FALSE, message=FALSE}
# Function to create a recipe
create_recipe <- function(response_var) {
  recipe(
    as.formula(paste(response_var, "~ .")),
    data = player_train) %>%    
    # Assign roles
    update_role(player_id, player_name, season, week, game_id, game_date, new_role = "id") %>%
    # Set Other Outcomes to non predictors
    step_rm(setdiff(response_vars, response_var)) %>%
    # Handle categorical data
    step_mutate(across(all_nominal_predictors(), ~ as.factor(.))) %>%
    step_novel(all_nominal_predictors()) %>%
    step_dummy(all_nominal_predictors(), -all_outcomes(), one_hot = FALSE) %>%
    # Remove near-zero variance features
    step_zv(all_predictors(), skip = TRUE) %>%
    step_nzv(all_predictors(), skip = TRUE) %>%
    step_corr(all_numeric_predictors(), threshold = .9) %>%
    step_YeoJohnson(all_numeric_predictors()) %>%
    step_normalize(all_numeric_predictors())
}
```

## PCA

```{r, message=FALSE, warning=FALSE}
# Function to create a recipe
create_recipe_pca <- function(response_var) {
  recipe(
    as.formula(paste(response_var, "~ .")),
    data = player_train) %>%    
    # Assign roles
    update_role(player_id, player_name, season, week, game_id, game_date, new_role = "id") %>%
    # Set Other Outcomes to non predictors
    update_role(setdiff(response_vars, response_var), new_role = "na") %>%
    # Handle categorical data
    step_mutate(across(all_nominal_predictors(), ~ as.factor(.))) %>%
    step_novel(all_nominal_predictors()) %>%
    step_dummy(all_nominal_predictors(), -all_outcomes(), one_hot = FALSE) %>%
    # Remove near-zero variance features
    step_zv(all_predictors(), skip = TRUE) %>%
    step_nzv(all_predictors(), skip = TRUE) %>%
    step_corr(all_numeric_predictors(), threshold = .9) %>%
    step_YeoJohnson(all_numeric_predictors()) %>%
    step_normalize(all_numeric_predictors()) %>%
    step_pca(all_numeric_predictors(), threshold = .95, id = "pca")
}

run_pca <- function(recipe) {
  # Prepare the recipe
  pca_prep <- prep(recipe)
  
  # Get PCA loadings
  pca_loadings <- tidy(pca_prep, id = "pca") %>%
    pivot_wider(names_from = component, values_from = value)
  
  # Get PCA variance
  pca_variance <- tidy(pca_prep, id = "pca", type = "variance") %>%
    pivot_wider(names_from = terms, values_from = value) %>%
    clean_names()
  
  # Return results as a list
  return(list(
    recipe = recipe,
    prep = pca_prep,
    loadings = pca_loadings,
    variance = pca_variance
    ))
}

# Function to create and process PCA for each response variable
run_pca_for_one_response <- function(response_var) {
  # Step 1: Create the recipe for the given response variable
  recipe <- create_recipe_pca(response_var)
  
  # Step 2: Run PCA using the recipe
  pca_results <- run_pca(recipe)
  
  # Step 3: Return the PCA results
  list(
    response_var = response_var,
    recipe = pca_results$recipe,
    prep = pca_results$prep,
    loadings = pca_results$loadings,
    variance = pca_results$variance
  )
}
# Run PCA for one response variable since outcome doesn't change loadings
pca_results <- run_pca_for_one_response("pass_attempts")
```

```{r, warning=FALSE, message=FALSE}
# View Loadings and Variance
pca_results$loadings %>%
  mutate(across(where(is.numeric), round, digits = 2))
pca_results$variance %>%
  mutate(across(where(is.numeric), round, digits = 2))

# Scree plot
pca_results$variance %>%
  ggplot(aes(x = component, y = percent_variance)) +
  geom_col(fill = "skyblue") +
  theme_bw() +
  labs(
    title = "Scree Plot",
    x = "Principal Component (PC)",
    y = "% Variance Explained"
  )

# Biplot of All Players together
juice(pca_results$prep) %>%
  ggplot(aes(PC01, PC02)) +
  geom_point(aes(color = as.factor(player_name)), alpha = 0.7, size = 2) + # Adjust variable as necessary
  theme_bw() +
  labs(
    title = "PC1 vs PC2",
    x = "PC1",
    y = "PC2",
    color = "Player Name"
  )

# Biplot of players separately
player_names <- unique(juice(pca_results$prep)$player_name)

# For loop to generate and save plots
for (player in player_names) {
  # Filter data for the current player
  player_data <- juice(pca_results$prep) %>%
    filter(player_name == player)
  
  # Create the plot
  p <- player_data %>%
    ggplot(aes(PC01, PC02)) +
    geom_point(color = "blue", alpha = 0.7, size = 2) +
    theme_bw() +
    labs(
      title = paste("PC1 vs PC2 for", player),
      x = "PC1",
      y = "PC2"
    )
  print(p)
}
```

## Linear Regression

```{r, eval=FALSE}
# Function to create recipe, fit the model, and return results
fit_linear_model <- function(response_var) {
  # Create the recipe for the given response variable
  rec <- create_recipe(response_var)
  
  # Linear regression specification
  linear_reg_spec <- linear_reg() %>%
    set_engine("lm")
  
  # Create the workflow
  linear_workflow <- workflow() %>%
    add_recipe(rec) %>%
    add_model(linear_reg_spec)
  
  # Fit resamples
  fit_resample_results <- fit_resamples(
    linear_workflow,
    resamples = player_folds,
    metrics = metric_set(rmse, mae, rsq)
  )
  
  # Collect metrics and predictions from fit_resamples
  fit_resamples_metrics <- collect_metrics(fit_resample_results)

  # Fit on training/testing split using last_fit
  last_fit_results <- last_fit(linear_workflow, player_splits, metric_set(rmse, mae, rsq))
  
  # Collect metrics and predictions from last_fit
  last_fit_metrics <- collect_metrics(last_fit_results)
  last_fit_predictions <- collect_predictions(last_fit_results)
  
  # Extract the fitted workflow from last_fit results
  fitted_workflow <- extract_workflow(last_fit_results)
  
  # Extract coefficients from the fitted model
  coefficients <- tidy(extract_fit_parsnip(fitted_workflow)$fit) %>%
    mutate(
      estimate = round(estimate, 2),
      std.error = round(std.error, 2),
      p.value = round(p.value, 3)
    )
  
  # Return a list of metrics, predictions, and rounded coefficients
  return(list(
    last_fit_metrics = last_fit_metrics,
    fit_resamples_metrics = fit_resamples_metrics,
    last_fit_predictions = last_fit_predictions,
    coefficients = coefficients
  ))
}
```

### Results

```{r, eval=FALSE}
# Set up parallel backend
cl <- makeCluster(20)
registerDoParallel(cl)
tic()
set.seed(6341)
# Perform parallel processing for linear regression
linear_results <- foreach(
    resp = response_vars,
    .packages = c("tidymodels", "recipes", "tune", "yardstick", "workflows", "dplyr", "broom"),
    .combine = 'c'  # Combine results into a named list
) %dopar% {
    # Fit the model with the current response variable and its corresponding optimal variables
    result <- fit_linear_model(resp)
    # Name the result by the response variable
    setNames(list(result), resp)
}
toc()
# Stop the parallel backend
stopCluster(cl)
registerDoSEQ()

combined_metrics <- list()
# Iterate over each response variable in linear_results
for (response_var in names(linear_results)) {
  # Extract last_fit_metrics for the current response variable
  metrics <- linear_results[[response_var]]$last_fit_metrics
  # Add a new column to denote the response variable
  metrics$response_var <- response_var
  # Store the modified metrics in the list
  combined_metrics[[response_var]] <- metrics
  }

# Combine all the metrics into a single data frame
final_metrics <- do.call(rbind, combined_metrics)
final_metrics %>%
  mutate(.estimate = round(.estimate, 2))

saveRDS(linear_results, "linear_results.rds")
```

```{r, eval=FALSE}
linear_results <- readRDS("linear_results.rds")
# Function to filter coefficients by p-value threshold
get_significant_predictors <- function(coef_table, threshold = 0.05) {
  coef_table %>%
    filter(p.value <= threshold) %>%  # Filter rows where p.value <= 0.05
    arrange(p.value)                 # Sort by p.value in ascending order
}

# Extract significant predictors for each response variable
significant_predictors <- lapply(linear_results, function(result) {
  get_significant_predictors(result$coefficients)
})

# Add names to the list for clarity
names(significant_predictors) <- names(linear_results)

significant_terms <- lapply(linear_results, function(result) {
  result$coefficients %>%
    filter(term != "(Intercept)") %>%  # Exclude (Intercept)
    filter(p.value <= 0.05) %>%  # Filter significant predictors
    pull(term)                   # Extract the term (predictor name)
})

saveRDS(significant_terms, "significant_terms.rds") # Rerun the model
```

```{r, eval=FALSE}
create_recipe_lm_new <- function(response_var, significant_terms) {
  # Retrieve significant terms for the given response variable
  predictors <- significant_terms[[response_var]]
  
  recipe(
    as.formula(paste(response_var, "~ .")),
    data = player_train
  ) %>%
    # Assign roles to identifiers (retain them in the data)
    update_role(player_id, player_name, season, week, game_id, game_date, new_role = "id") %>%
    # Remove predictors not in significant_terms for this response variable (but keep identifiers)
    step_rm(setdiff(names(player_train), c(predictors, response_var,
                                           "player_id", "player_name", "season", 
                                           "week", "game_id", "game_date"))) %>%
    # Handle categorical data
    step_mutate(across(all_nominal_predictors(), ~ as.factor(.))) %>%
    step_novel(all_nominal_predictors()) %>%
    step_dummy(all_nominal_predictors(), -all_outcomes(), one_hot = FALSE) %>%
    # Remove near-zero variance features
    step_zv(all_predictors(), skip = TRUE) %>%
    step_nzv(all_predictors(), skip = TRUE) %>%
    step_corr(all_numeric_predictors(), threshold = .9) %>%
    step_YeoJohnson(all_numeric_predictors()) %>%
    step_normalize(all_numeric_predictors())
}

# Function to create recipe, fit the model, and return results
fit_linear_model_new <- function(response_var, significant_terms) {
  # Create the recipe for the given response variable
  rec <- create_recipe_lm_new(response_var, significant_terms)
  
  # Linear regression specification
  linear_reg_spec <- linear_reg() %>%
    set_engine("lm")
  
  # Create the workflow
  linear_workflow <- workflow() %>%
    add_recipe(rec) %>%
    add_model(linear_reg_spec)
  
  # Fit resamples
  fit_resample_results <- fit_resamples(
    linear_workflow,
    resamples = player_folds,
    metrics = metric_set(rmse, mae, rsq)
  )
  
  # Collect metrics and predictions from fit_resamples
  fit_resamples_metrics <- collect_metrics(fit_resample_results)

  # Fit on training/testing split using last_fit
  last_fit_results <- last_fit(linear_workflow, player_splits, metric_set(rmse, mae, rsq))
  
  # Collect metrics and predictions from last_fit
  last_fit_metrics <- collect_metrics(last_fit_results)
  last_fit_predictions <- collect_predictions(last_fit_results)
  
  # Extract the fitted workflow from last_fit results
  fitted_workflow <- extract_workflow(last_fit_results)
  
  # Extract coefficients from the fitted model
  coefficients <- tidy(extract_fit_parsnip(fitted_workflow)$fit) %>%
    mutate(
      estimate = round(estimate, 2),
      std.error = round(std.error, 2),
      p.value = round(p.value, 3)
    )
  
  # Return a list of metrics, predictions, and rounded coefficients
  return(list(
    last_fit_metrics = last_fit_metrics,
    fit_resamples_metrics = fit_resamples_metrics,
    last_fit_predictions = last_fit_predictions,
    coefficients = coefficients,
    recipe = rec,
    spec = linear_reg_spec,
    workflow = linear_workflow
  ))
}

# Set up parallel backend
cl <- makeCluster(20)
registerDoParallel(cl)
tic()
set.seed(6341)
# Perform parallel processing for linear regression
linear_results_new <- foreach(
    resp = response_vars,
    .packages = c("tidymodels", "recipes", "tune", "yardstick", "workflows", "dplyr", "broom"),
    .combine = 'c'  # Combine results into a named list
) %dopar% {
    # Fit the model with the current response variable and its corresponding significant terms
    result <- fit_linear_model_new(resp, significant_terms)
    
    # Name the result by the response variable
    setNames(list(result), resp)
}
toc()
# Stop the parallel backend
stopCluster(cl)
registerDoSEQ()
saveRDS(linear_results_new, "linear_results_new.rds")
```

```{r, eval=FALSE}
linear_results_new <- readRDS("linear_results_new.rds")
combined_metrics_new <- list()
# Iterate over each response variable in linear_results
for (response_var in names(linear_results_new)) {
  # Extract last_fit_metrics for the current response variable
  metrics_new <- linear_results_new[[response_var]]$last_fit_metrics
  # Add a new column to denote the response variable
  metrics_new$response_var <- response_var
  # Store the modified metrics in the list
  combined_metrics_new[[response_var]] <- metrics_new
  }

# Combine all the metrics into a single data frame
final_metrics_new <- do.call(rbind, combined_metrics_new)
final_metrics_new %>%
  mutate(.estimate = round(.estimate, 2))
```

## Penalized Regression

```{r, warning=FALSE, message=FALSE, eval=FALSE}
# Function to build and evaluate the model
run_elastic_net_model <- function(response_var) {
  recipe <- create_recipe(response_var)

  # Define a elastic_net regression specification
  elastic_net_spec <- linear_reg(penalty = tune(), mixture = tune()) %>%  # Fixed mixture = 1 for pure Lasso
    set_engine("glmnet") %>%
    set_mode("regression")

  # Define the workflow
  elastic_net_workflow <- workflow() %>%
    add_recipe(recipe) %>%
    add_model(elastic_net_spec)
  
  # Set up tuning grid
  penalty_grid <- grid_regular(
    penalty(range = c(-4, 2)),
    mixture(range = c(0, 1)),    # Linear scale for mixture (0 = Ridge, 1 = Lasso)
    levels = c(10, 5)            # 10 levels for penalty, 5 for mixture
  )
  
  tune_control <- control_grid(save_pred = TRUE, parallel_over = "resamples")
  
  set.seed(6341)
  # Perform tuning with resamples
  tuning_results <- tune_grid(
    elastic_net_workflow,
    resamples = player_folds,
    grid = penalty_grid,
    metrics = metric_set(rmse, mae, rsq),
    control = tune_control)
  
  # Select the best penalty based on RMSE
  best_penalty <- select_best(tuning_results, metric = "rmse")
  
  # Finalize the workflow with the best penalty
  finalized_workflow <- finalize_workflow(elastic_net_workflow, best_penalty)
  
  # Evaluate the finalized workflow on player_folds using `fit_resamples()`
  resample_control <- control_resamples(save_pred = TRUE, parallel_over = "resamples")
  
  set.seed(6341)
  resample_results <- fit_resamples(
    finalized_workflow,
    resamples = player_folds,
    metrics = metric_set(rmse, mae, rsq),
    control = resample_control
  )
  
  # Evaluate the finalized model on the test set using `last_fit()`
  last_fit_result <- last_fit(
    finalized_workflow,
    player_splits,
    metric_set(rmse, mae, rsq)
  )
  
  # Collect metrics and predictions
  elastic_net_results <- list(
    resample_metrics = collect_metrics(resample_results),
    resample_predictions = collect_predictions(resample_results),
    best_penalty = best_penalty,
    final_metrics = collect_metrics(last_fit_result),
    final_predictions = collect_predictions(last_fit_result),
    spec = elastic_net_spec,
    workflow = finalized_workflow,
    coefficients = extract_workflow(last_fit_result) %>%
      extract_fit_parsnip() %>%
      tidy()
  )
  return(elastic_net_results)
}
```

### Results

```{r, eval=FALSE}
tic()
set.seed(6341)
cl <- makeCluster(20)
registerDoParallel(cl)
elastic_net_results <- foreach(
    resp = response_vars,
    .packages = c("tidymodels", "recipes", "tune", "glmnet", "yardstick", "workflows"),
    .combine = 'c'
) %dopar% {
    result <- run_elastic_net_model(resp)
    setNames(list(result), resp)  # Ensure results are named by 'resp'
}
stopCluster(cl)
registerDoSEQ()
toc() # ~9 seconds

names(elastic_net_results) <- response_vars
saveRDS(elastic_net_results, "elastic_net_results.rds")
```

```{r, eval=FALSE}
# Load Results
elastic_net_results <- readRDS("elastic_net_results.rds")
elastic_net_results$pass_attempts$
# Combine metrics for all responses
combined_metrics <- bind_rows(
  lapply(response_vars, function(resp) {
    final_metrics <- elastic_net_results[[resp]]$final_metrics %>%
      mutate(metric_type = "final")
    
    # Combine and add the response variable
    bind_rows(final_metrics) %>%
      mutate(response = resp)
  })
)

# View the combined metrics
combined_metrics %>%
  select(response, .metric, .estimate) %>%
  mutate(.estimate = round(.estimate, 2))

# Display the combined table for elastic net results
print(elastic_net_combined_metrics)
```

## Random Forest

```{r, eval=FALSE}
fit_random_forest <- function(response_var) {
  # Create the recipe for the given response variable
  rec <- create_recipe_pca(response_var)
  
  # Random Forest specification with tuning
  rf_spec <- rand_forest(
    mtry = 63, 
    min_n = tune(), 
    trees = tune()
  ) %>%
    set_engine("ranger") %>%
    set_mode("regression")
  
  # Create the workflow
  rf_workflow <- workflow() %>%
    add_recipe(rec) %>%
    add_model(rf_spec)
  
  # Tuning grid
  rf_tuning_grid <- grid_random(
    min_n(range = c(10, 50)),     # Minimum number of samples in terminal nodes
    trees(range = c(500, 1000)),  # Number of trees
    size = 30                     # 50 random combinations
)
  tune_control <- control_grid(save_pred = TRUE, parallel_over = "resamples")

  # Perform tuning using temporal cross-validation
  rf_tune_results <- tune_grid(
    rf_workflow,
    resamples = player_folds,  # Temporal CV folds
    grid = rf_tuning_grid,
    metrics = metric_set(rmse, rsq),
    control = tune_control
  )
  
  # Select the best hyperparameters
  best_params <- select_best(rf_tune_results, metric = "rmse")
  
  # Finalize the workflow with the best hyperparameters
  final_rf <- finalize_workflow(rf_workflow, best_params)
  
  # Fit the finalized model on the training/testing split
  last_fit_results <- last_fit(final_rf, player_splits)
  
  # Collect metrics and predictions
  last_fit_metrics <- collect_metrics(last_fit_results)
  last_fit_predictions <- collect_predictions(last_fit_results)
  
  rf_fit <- fit(final_rf, data = player_train)
  
  return(list(
    last_fit_metrics = last_fit_metrics,
    last_fit_predictions = last_fit_predictions,
    best_params = best_params,
    tune_results = rf_tune_results,
    rf_fit = rf_fit
  ))
}
cl <- makeCluster(30)
registerDoParallel(cl)
```

### Results

```{r, eval=FALSE}
# Parallel execution for random forest tuning
cl <- makeCluster(8)
registerDoParallel(cl)
set.seed(6341)
rf_results <- foreach(
  resp = response_vars,
  .packages = c("tidymodels", "tune", "recipes", "yardstick", "workflows", "ranger"),
  .combine = 'c'
) %dopar% {
  tryCatch({
    result <- fit_random_forest(resp)
    setNames(list(result), resp)
  }, error = function(e) {
    message(sprintf("Error in response variable %s: %s", resp, e))
    NULL
  })
}

saveRDS(rf_results, "rf_results")

# Clean up the parallel backend
stopCluster(cl)
registerDoSEQ()
```

Chosen Models for Each Reponse Variable:\
Linear Regression:

-   Passing Yards

Elastic Net:

-   Pass Attempts

-   Passing TDs

-   Completions

-   Ints

-   Rush Attempts

-   Rushing Yards

-   Rushing TDs

# Display All Results
## Coefficients
```{r}
# Put response variables in a vector
response_vars <- c("pass_attempts", "completions", "passing_yards", 
                   "rush_attempts", "rushing_yards", "passing_tds", 
                   "interceptions", "rushing_tds")
significant_terms <- readRDS("significant_terms.rds")
print(significant_terms)

linear_results_new <- readRDS("linear_results_new.rds")
elastic_net_results <- readRDS("elastic_net_results.rds")
rf_results <- readRDS("rf_results.rds")

linear_coefficients <- bind_rows(
  lapply(response_vars, function(resp) {
    linear_results_new[[resp]]$coefficients %>%
      mutate(response = resp)
  })
)

# Top coefficient for each response from linear coefficients
top_linear_coefficients <- linear_coefficients %>%
  filter(p.value <= .05) %>%                        # Filter significant coefficients
  filter(term != "(Intercept)") %>%                # Exclude intercept
  mutate(statistic = round(statistic, 2)) %>%      # Round statistic for clarity
  arrange(term, response, desc(abs(estimate))) %>% # Sort by term and response
  group_by(response) %>%                           # Group by response
  slice_max(order_by = abs(estimate), n = 1) %>%   # Select top absolute estimate
  ungroup()

# Display the top linear coefficients with kable
top_linear_coefficients %>%
  kable(
    col.names = c("Term", "Estimate", "Std. Error", "Statistic", "P-Value", "Response"),
    format = "html",
    caption = "Top Coefficient for Each Response (Linear Model)"
  ) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width = FALSE
  )                                      # Ungroup for final output


elastic_net_coefficients <- bind_rows(
  lapply(response_vars, function(resp) {
    elastic_net_results[[resp]]$coefficients %>%
      mutate(response = resp)
  })
)

# Top coefficient for each response from elastic net coefficients
top_elastic_net_coefficients <- elastic_net_coefficients %>%
  filter(term != "(Intercept)") %>%                # Exclude intercept
  arrange(response, desc(abs(estimate))) %>%       # Sort by response and abs(estimate)
  group_by(response) %>%                           # Group by response
  slice_max(order_by = abs(estimate), n = 1) %>%   # Select top absolute estimate
  mutate(estimate = round(estimate, 2),
         penalty = round(penalty, 2)) %>%
  ungroup()

# Display the top elastic net coefficients with kable
top_elastic_net_coefficients %>%
  kable(
    col.names = c("Term", "Estimate", "Penalty", "Response"),
    format = "html",
    caption = "Top Coefficient for Each Response (Elastic Net Model)"
  ) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width = FALSE
  )
```
## Best Parameters
```{r}
elastic_net_best_penalty <- bind_rows(
  lapply(response_vars, function(resp) {
    tibble(
      penalty = elastic_net_results[[resp]]$best_penalty$penalty, # Extract the penalty value
      response = resp
    )
  })
)

# Display the elastic net best penalty
print(elastic_net_best_penalty)

rf_best_params <- bind_rows(
  lapply(response_vars, function(resp) {
    rf_results[[resp]]$best_params %>%
      mutate(response = resp)
  })
)

# Display the random forest best parameters
print(rf_best_params)

```
## Metrics
```{r}
# Function to process linear_results_new
process_linear_results <- function(model_results, response) {
  last_fit <- model_results[[response]]$last_fit_metrics %>%
    select(.metric, .estimate) %>%
    rename(Metric = .metric, Estimate = .estimate)
  
  resample <- model_results[[response]]$fit_resamples_metrics %>%
    select(.metric, mean, std_err) %>%
    rename(Metric = .metric, CV_Avg = mean, SE = std_err)
  
  combined <- last_fit %>%
    left_join(resample, by = "Metric") %>%
    mutate(Response = response)
  
  return(combined)
}

# Function to process elastic_net_results
process_elastic_net_results <- function(model_results, response) {
  final_metrics <- model_results[[response]]$final_metrics %>%
    select(.metric, .estimate) %>%
    rename(Metric = .metric, Estimate = .estimate)
  
  resample_metrics <- model_results[[response]]$resample_metrics %>%
    select(.metric, mean, std_err) %>%
    rename(Metric = .metric, CV_Avg = mean, SE = std_err)
  
  combined <- final_metrics %>%
    left_join(resample_metrics, by = "Metric") %>%
    mutate(Response = response)
  
  return(combined)
}

# Function to process rf_results
process_rf_results <- function(model_results, response) {
  last_fit <- model_results[[response]]$last_fit_metrics %>%
    select(.metric, .estimate) %>%
    rename(Metric = .metric, Estimate = .estimate)
  
  combined <- last_fit %>%
    mutate(Response = response, CV_Avg = NA, SE = NA)
  
  return(combined)
}

# Process all results for each model type
process_all_results <- function(model_results, model_name, process_function) {
  results <- lapply(response_vars, function(response) {
    process_function(model_results, response)
  }) %>%
    bind_rows() %>%
    mutate(Model = model_name) # Add model name
  
  return(results)
}

# Process each model
linear_metrics <- process_all_results(linear_results_new, "Linear Regression", process_linear_results) %>%
  select(Response, Metric, Estimate, CV_Avg, SE) %>%
  mutate(across(where(is.numeric), ~ round(.x, 2)))
elastic_net_metrics <- process_all_results(elastic_net_results, "Elastic Net", process_elastic_net_results) %>%
  select(Response, Metric, Estimate, CV_Avg, SE) %>%
  mutate(across(where(is.numeric), ~ round(.x, 2)))
rf_metrics <- process_all_results(rf_results, "Random Forest", process_rf_results) %>%
  select(Response, Metric, Estimate, CV_Avg, SE) %>%
  mutate(across(where(is.numeric), ~ round(.x, 2)))

# Linear Regression Results Table
linear_metrics %>%
  gt() %>%
  tab_header(
    title = "Linear Regression Results",
    subtitle = "Metrics Summary for Response Variables"
  ) %>%
  cols_label(
    Response = "Response Variable",
    Metric = "Metric Type",
    Estimate = "Estimate",
    CV_Avg = "Cross-Validation Avg",
    SE = "Standard Error"
  ) %>%
  fmt_number(columns = c(Estimate, CV_Avg, SE), decimals = 2) %>%
  tab_options(
    table.font.size = 12,
    heading.title.font.size = 14,
    heading.subtitle.font.size = 12
  )

# Elastic Net Results Table
elastic_net_metrics %>%
  gt() %>%
  tab_header(
    title = "Elastic Net Results",
    subtitle = "Metrics Summary for Response Variables"
  ) %>%
  cols_label(
    Response = "Response Variable",
    Metric = "Metric Type",
    Estimate = "Estimate",
    CV_Avg = "Cross-Validation Avg",
    SE = "Standard Error"
  ) %>%
  fmt_number(columns = c(Estimate, CV_Avg, SE), decimals = 2) %>%
  tab_options(
    table.font.size = 12,
    heading.title.font.size = 14,
    heading.subtitle.font.size = 12
  )

# Random Forest Results Table
rf_metrics %>%
  gt() %>%
  tab_header(
    title = "Random Forest Results",
    subtitle = "Metrics Summary for Response Variables"
  ) %>%
  cols_label(
    Response = "Response Variable",
    Metric = "Metric Type",
    Estimate = "Estimate",
    CV_Avg = "Cross-Validation Avg",
    SE = "Standard Error"
  ) %>%
  fmt_number(columns = c(Estimate, CV_Avg, SE), decimals = 2) %>%
  tab_options(
    table.font.size = 12,
    heading.title.font.size = 14,
    heading.subtitle.font.size = 12
  )
```